; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s
; RUN: llc -mtriple=loongarch64 -mattr=+lasx < %s | FileCheck %s

define <32 x i8> @vsadd_b(<32 x i8> %a, <32 x i8> %b) {
; CHECK-LABEL: vsadd_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvadd.b $xr2, $xr0, $xr1
; CHECK-NEXT:    xvslt.b $xr0, $xr2, $xr0
; CHECK-NEXT:    xvslti.b $xr1, $xr1, 0
; CHECK-NEXT:    xvxor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvsrai.b $xr1, $xr2, 7
; CHECK-NEXT:    xvbitrevi.b $xr1, $xr1, 7
; CHECK-NEXT:    xvbitsel.v $xr0, $xr2, $xr1, $xr0
; CHECK-NEXT:    ret
  %ret = call <32 x i8> @llvm.sadd.sat.v16i8(<32 x i8> %a, <32 x i8> %b)
  ret <32 x i8> %ret
}

define <16 x i16> @vsadd_h(<16 x i16> %a, <16 x i16> %b) {
; CHECK-LABEL: vsadd_h:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvadd.h $xr2, $xr0, $xr1
; CHECK-NEXT:    xvslt.h $xr0, $xr2, $xr0
; CHECK-NEXT:    xvslti.h $xr1, $xr1, 0
; CHECK-NEXT:    xvxor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvsrai.h $xr1, $xr2, 15
; CHECK-NEXT:    xvbitrevi.h $xr1, $xr1, 15
; CHECK-NEXT:    xvbitsel.v $xr0, $xr2, $xr1, $xr0
; CHECK-NEXT:    ret
  %ret = call <16 x i16> @llvm.sadd.sat.v8i32(<16 x i16> %a, <16 x i16> %b)
  ret <16 x i16> %ret
}

define <8 x i32> @vsadd_w(<8 x i32> %a, <8 x i32> %b) {
; CHECK-LABEL: vsadd_w:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvadd.w $xr2, $xr0, $xr1
; CHECK-NEXT:    xvslt.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvslti.w $xr1, $xr1, 0
; CHECK-NEXT:    xvxor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvsrai.w $xr1, $xr2, 31
; CHECK-NEXT:    xvbitrevi.w $xr1, $xr1, 31
; CHECK-NEXT:    xvbitsel.v $xr0, $xr2, $xr1, $xr0
; CHECK-NEXT:    ret
  %ret = call <8 x i32> @llvm.sadd.sat.v4i64(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %ret
}

define <4 x i64> @vsadd_d(<4 x i64> %a, <4 x i64> %b) {
; CHECK-LABEL: vsadd_d:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvadd.d $xr2, $xr0, $xr1
; CHECK-NEXT:    xvslt.d $xr0, $xr2, $xr0
; CHECK-NEXT:    xvslti.d $xr1, $xr1, 0
; CHECK-NEXT:    xvxor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvsrai.d $xr1, $xr2, 63
; CHECK-NEXT:    xvbitrevi.d $xr1, $xr1, 63
; CHECK-NEXT:    xvbitsel.v $xr0, $xr2, $xr1, $xr0
; CHECK-NEXT:    ret
  %ret = call <4 x i64> @llvm.sadd.sat.v2i128(<4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %ret
}

declare <32 x i8> @llvm.sadd.sat.v16i8(<32 x i8>, <32 x i8>)
declare <16 x i16> @llvm.sadd.sat.v8i16(<16 x i16>, <16 x i16>)
declare <8 x i32> @llvm.sadd.sat.v4i32(<8 x i32>, <8 x i32>)
declare <4 x i64> @llvm.sadd.sat.v2i64(<4 x i64>, <4 x i64>)
